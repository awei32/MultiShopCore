# 用户模块部署设计文档

## 一、部署架构概述

### 1.1 部署目标

用户模块的部署设计旨在实现：

- **高可用性**：99.9%以上的服务可用性
- **可扩展性**：支持水平扩展，应对业务增长
- **安全性**：多层安全防护，保障数据安全
- **可维护性**：简化运维操作，支持快速部署和回滚
- **成本优化**：合理利用资源，降低运营成本

### 1.2 部署环境

| 环境 | 用途 | 配置 | 实例数量 |
|------|------|------|----------|
| 开发环境 | 开发测试 | 2C4G | 1 |
| 测试环境 | 集成测试 | 4C8G | 2 |
| 预生产环境 | 性能测试 | 8C16G | 3 |
| 生产环境 | 正式服务 | 16C32G | 5+ |

### 1.3 技术栈

- **容器化**：Docker + Kubernetes
- **服务网格**：Istio
- **监控**：Prometheus + Grafana
- **日志**：ELK Stack
- **CI/CD**：GitLab CI/CD
- **镜像仓库**：Harbor
- **配置管理**：Helm

## 二、容器化设计

### 2.1 Docker镜像构建

#### 2.1.1 多阶段构建Dockerfile

```dockerfile
# 构建阶段
FROM maven:3.8.4-openjdk-11-slim AS builder

# 设置工作目录
WORKDIR /app

# 复制pom文件，利用Docker缓存
COPY pom.xml .
COPY src ./src

# 构建应用
RUN mvn clean package -DskipTests -Dmaven.repo.local=/app/.m2

# 运行阶段
FROM openjdk:11-jre-slim

# 安装必要工具
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        curl \
        jq \
        ca-certificates && \
    rm -rf /var/lib/apt/lists/*

# 创建应用用户
RUN groupadd -r multishop && \
    useradd -r -g multishop -d /app -s /sbin/nologin multishop

# 设置工作目录
WORKDIR /app

# 复制应用文件
COPY --from=builder /app/target/user-service-*.jar app.jar

# 创建日志目录
RUN mkdir -p /app/logs && \
    chown -R multishop:multishop /app

# 切换到应用用户
USER multishop

# 健康检查
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
    CMD curl -f http://localhost:8080/actuator/health || exit 1

# 暴露端口
EXPOSE 8080

# JVM参数优化
ENV JAVA_OPTS="-Xms512m -Xmx2g -XX:+UseG1GC -XX:+UseContainerSupport -XX:MaxRAMPercentage=75.0"

# 启动命令
ENTRYPOINT ["sh", "-c", "java $JAVA_OPTS -jar app.jar"]
```

#### 2.1.2 镜像构建脚本

```bash
#!/bin/bash
# build-image.sh

set -e

# 配置变量
IMAGE_NAME="multishop/user-service"
VERSION=${1:-latest}
REGISTRY="harbor.multishop.com"

# 构建镜像
echo "构建Docker镜像..."
docker build -t ${IMAGE_NAME}:${VERSION} .

# 标记镜像
docker tag ${IMAGE_NAME}:${VERSION} ${REGISTRY}/${IMAGE_NAME}:${VERSION}
docker tag ${IMAGE_NAME}:${VERSION} ${REGISTRY}/${IMAGE_NAME}:latest

# 推送镜像
echo "推送镜像到仓库..."
docker push ${REGISTRY}/${IMAGE_NAME}:${VERSION}
docker push ${REGISTRY}/${IMAGE_NAME}:latest

# 清理本地镜像
docker rmi ${IMAGE_NAME}:${VERSION}

echo "镜像构建完成: ${REGISTRY}/${IMAGE_NAME}:${VERSION}"
```

### 2.2 镜像安全扫描

#### 2.2.1 安全扫描配置

```yaml
# .gitlab-ci.yml 安全扫描阶段
security_scan:
  stage: security
  image: aquasec/trivy:latest
  script:
    - trivy image --exit-code 1 --severity HIGH,CRITICAL $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
  allow_failure: false
  only:
    - main
    - develop

# Dockerfile安全最佳实践检查
hadolint:
  stage: lint
  image: hadolint/hadolint:latest-debian
  script:
    - hadolint Dockerfile
  only:
    - merge_requests
    - main
```

## 三、Kubernetes部署配置

### 3.1 命名空间配置

```yaml
# namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: multishop-user
  labels:
    name: multishop-user
    environment: production
    team: backend
---
# 资源配额
apiVersion: v1
kind: ResourceQuota
metadata:
  name: user-service-quota
  namespace: multishop-user
spec:
  hard:
    requests.cpu: "10"
    requests.memory: 20Gi
    limits.cpu: "20"
    limits.memory: 40Gi
    pods: "20"
    services: "10"
    persistentvolumeclaims: "5"
---
# 限制范围
apiVersion: v1
kind: LimitRange
metadata:
  name: user-service-limits
  namespace: multishop-user
spec:
  limits:
  - default:
      cpu: "1"
      memory: "2Gi"
    defaultRequest:
      cpu: "500m"
      memory: "1Gi"
    type: Container
```

### 3.2 配置管理

#### 3.2.1 ConfigMap配置

```yaml
# configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-service-config
  namespace: multishop-user
data:
  application.yml: |
    server:
      port: 8080
      servlet:
        context-path: /api/user
    
    spring:
      profiles:
        active: ${SPRING_PROFILES_ACTIVE:prod}
      
      datasource:
        url: jdbc:mysql://${DB_HOST}:${DB_PORT}/${DB_NAME}?useSSL=true&serverTimezone=UTC
        username: ${DB_USERNAME}
        driver-class-name: com.mysql.cj.jdbc.Driver
        hikari:
          maximum-pool-size: 20
          minimum-idle: 5
          connection-timeout: 30000
          idle-timeout: 600000
          max-lifetime: 1800000
      
      redis:
        host: ${REDIS_HOST}
        port: ${REDIS_PORT}
        timeout: 5000ms
        lettuce:
          pool:
            max-active: 20
            max-idle: 10
            min-idle: 5
    
    management:
      endpoints:
        web:
          exposure:
            include: health,info,metrics,prometheus
      endpoint:
        health:
          show-details: when-authorized
      metrics:
        export:
          prometheus:
            enabled: true
    
    logging:
      level:
        com.multishop: INFO
        org.springframework.security: WARN
      pattern:
        console: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"
        file: "%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n"
      file:
        name: /app/logs/application.log
        max-size: 100MB
        max-history: 30

  logback-spring.xml: |
    <?xml version="1.0" encoding="UTF-8"?>
    <configuration>
        <include resource="org/springframework/boot/logging/logback/defaults.xml"/>
        
        <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
            <encoder>
                <pattern>${CONSOLE_LOG_PATTERN}</pattern>
            </encoder>
        </appender>
        
        <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
            <file>/app/logs/application.log</file>
            <rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
                <fileNamePattern>/app/logs/application.%d{yyyy-MM-dd}.%i.log</fileNamePattern>
                <maxFileSize>100MB</maxFileSize>
                <maxHistory>30</maxHistory>
                <totalSizeCap>3GB</totalSizeCap>
            </rollingPolicy>
            <encoder>
                <pattern>${FILE_LOG_PATTERN}</pattern>
            </encoder>
        </appender>
        
        <root level="INFO">
            <appender-ref ref="CONSOLE"/>
            <appender-ref ref="FILE"/>
        </root>
    </configuration>
```

#### 3.2.2 Secret配置

```yaml
# secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: user-service-secret
  namespace: multishop-user
type: Opaque
data:
  # Base64编码的敏感信息
  db-password: <BASE64_ENCODED_DB_PASSWORD>
  redis-password: <BASE64_ENCODED_REDIS_PASSWORD>
  jwt-secret: <BASE64_ENCODED_JWT_SECRET>
  encryption-key: <BASE64_ENCODED_ENCRYPTION_KEY>
---
# TLS证书
apiVersion: v1
kind: Secret
metadata:
  name: user-service-tls
  namespace: multishop-user
type: kubernetes.io/tls
data:
  tls.crt: <BASE64_ENCODED_CERT>
  tls.key: <BASE64_ENCODED_KEY>
```

### 3.3 部署配置

#### 3.3.1 Deployment配置

```yaml
# deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service
  namespace: multishop-user
  labels:
    app: user-service
    version: v1
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: user-service
      version: v1
  template:
    metadata:
      labels:
        app: user-service
        version: v1
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/actuator/prometheus"
    spec:
      serviceAccountName: user-service-sa
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 1000
      containers:
      - name: user-service
        image: harbor.multishop.com/multishop/user-service:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        env:
        - name: SPRING_PROFILES_ACTIVE
          value: "prod"
        - name: DB_HOST
          value: "mysql-cluster.database.svc.cluster.local"
        - name: DB_PORT
          value: "3306"
        - name: DB_NAME
          value: "multishop_user"
        - name: DB_USERNAME
          value: "user_service"
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: user-service-secret
              key: db-password
        - name: REDIS_HOST
          value: "redis-cluster.cache.svc.cluster.local"
        - name: REDIS_PORT
          value: "6379"
        - name: REDIS_PASSWORD
          valueFrom:
            secretKeyRef:
              name: user-service-secret
              key: redis-password
        - name: JWT_SECRET
          valueFrom:
            secretKeyRef:
              name: user-service-secret
              key: jwt-secret
        - name: ENCRYPTION_KEY
          valueFrom:
            secretKeyRef:
              name: user-service-secret
              key: encryption-key
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1"
        livenessProbe:
          httpGet:
            path: /actuator/health/liveness
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /actuator/health/readiness
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        startupProbe:
          httpGet:
            path: /actuator/health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 30
        volumeMounts:
        - name: config-volume
          mountPath: /app/config
        - name: logs-volume
          mountPath: /app/logs
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          capabilities:
            drop:
            - ALL
      volumes:
      - name: config-volume
        configMap:
          name: user-service-config
      - name: logs-volume
        emptyDir: {}
      imagePullSecrets:
      - name: harbor-secret
      nodeSelector:
        node-type: application
      tolerations:
      - key: "application"
        operator: "Equal"
        value: "true"
        effect: "NoSchedule"
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - user-service
              topologyKey: kubernetes.io/hostname
```

#### 3.3.2 Service配置

```yaml
# service.yaml
apiVersion: v1
kind: Service
metadata:
  name: user-service
  namespace: multishop-user
  labels:
    app: user-service
  annotations:
    service.beta.kubernetes.io/aws-load-balancer-type: nlb
spec:
  type: ClusterIP
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP
    name: http
  selector:
    app: user-service
---
# Headless Service for StatefulSet (if needed)
apiVersion: v1
kind: Service
metadata:
  name: user-service-headless
  namespace: multishop-user
  labels:
    app: user-service
spec:
  clusterIP: None
  ports:
  - port: 8080
    targetPort: 8080
    protocol: TCP
    name: http
  selector:
    app: user-service
```

#### 3.3.3 Ingress配置

```yaml
# ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: user-service-ingress
  namespace: multishop-user
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    nginx.ingress.kubernetes.io/backend-protocol: "HTTP"
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/rate-limit-window: "1m"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
spec:
  tls:
  - hosts:
    - api.multishop.com
    secretName: user-service-tls
  rules:
  - host: api.multishop.com
    http:
      paths:
      - path: /user(/|$)(.*)
        pathType: Prefix
        backend:
          service:
            name: user-service
            port:
              number: 80
```

### 3.4 水平扩缩容配置

#### 3.4.1 HPA配置

```yaml
# hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: user-service-hpa
  namespace: multishop-user
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: user-service
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: http_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
      selectPolicy: Max
```

#### 3.4.2 VPA配置

```yaml
# vpa.yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: user-service-vpa
  namespace: multishop-user
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: user-service
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: user-service
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 2
        memory: 4Gi
      controlledResources: ["cpu", "memory"]
```

## 四、服务网格配置

### 4.1 Istio配置

#### 4.1.1 VirtualService配置

```yaml
# virtualservice.yaml
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: user-service-vs
  namespace: multishop-user
spec:
  hosts:
  - user-service
  - api.multishop.com
  gateways:
  - user-service-gateway
  - mesh
  http:
  - match:
    - uri:
        prefix: /api/user
    route:
    - destination:
        host: user-service
        port:
          number: 80
        subset: v1
      weight: 100
    timeout: 30s
    retries:
      attempts: 3
      perTryTimeout: 10s
      retryOn: 5xx,reset,connect-failure,refused-stream
    fault:
      delay:
        percentage:
          value: 0.1
        fixedDelay: 5s
```

#### 4.1.2 DestinationRule配置

```yaml
# destinationrule.yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: user-service-dr
  namespace: multishop-user
spec:
  host: user-service
  trafficPolicy:
    connectionPool:
      tcp:
        maxConnections: 100
      http:
        http1MaxPendingRequests: 50
        http2MaxRequests: 100
        maxRequestsPerConnection: 10
        maxRetries: 3
        consecutiveGatewayErrors: 5
        interval: 30s
        baseEjectionTime: 30s
        maxEjectionPercent: 50
    loadBalancer:
      simple: LEAST_CONN
    outlierDetection:
      consecutiveGatewayErrors: 5
      interval: 30s
      baseEjectionTime: 30s
      maxEjectionPercent: 50
      minHealthPercent: 50
  subsets:
  - name: v1
    labels:
      version: v1
    trafficPolicy:
      circuitBreaker:
        connectionPool:
          tcp:
            maxConnections: 50
          http:
            http1MaxPendingRequests: 25
            maxRequestsPerConnection: 5
```

#### 4.1.3 Gateway配置

```yaml
# gateway.yaml
apiVersion: networking.istio.io/v1beta1
kind: Gateway
metadata:
  name: user-service-gateway
  namespace: multishop-user
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    tls:
      mode: SIMPLE
      credentialName: user-service-tls
    hosts:
    - api.multishop.com
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - api.multishop.com
    tls:
      httpsRedirect: true
```

### 4.2 安全策略

#### 4.2.1 PeerAuthentication配置

```yaml
# peerauthentication.yaml
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: user-service-pa
  namespace: multishop-user
spec:
  selector:
    matchLabels:
      app: user-service
  mtls:
    mode: STRICT
```

#### 4.2.2 AuthorizationPolicy配置

```yaml
# authorizationpolicy.yaml
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: user-service-authz
  namespace: multishop-user
spec:
  selector:
    matchLabels:
      app: user-service
  rules:
  - from:
    - source:
        principals: ["cluster.local/ns/multishop-gateway/sa/gateway-service"]
    - source:
        namespaces: ["multishop-order", "multishop-product"]
    to:
    - operation:
        methods: ["GET", "POST", "PUT", "DELETE"]
        paths: ["/api/user/*"]
  - from:
    - source:
        principals: ["cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account"]
    to:
    - operation:
        methods: ["GET"]
        paths: ["/actuator/health"]
```

## 五、监控与日志

### 5.1 Prometheus监控配置

#### 5.1.1 ServiceMonitor配置

```yaml
# servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: user-service-monitor
  namespace: multishop-user
  labels:
    app: user-service
spec:
  selector:
    matchLabels:
      app: user-service
  endpoints:
  - port: http
    path: /actuator/prometheus
    interval: 30s
    scrapeTimeout: 10s
    honorLabels: true
```

#### 5.1.2 PrometheusRule配置

```yaml
# prometheusrule.yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: user-service-rules
  namespace: multishop-user
  labels:
    app: user-service
spec:
  groups:
  - name: user-service.rules
    rules:
    - alert: UserServiceDown
      expr: up{job="user-service"} == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "用户服务不可用"
        description: "用户服务 {{ $labels.instance }} 已经停止响应超过1分钟"
    
    - alert: UserServiceHighCPU
      expr: rate(process_cpu_seconds_total{job="user-service"}[5m]) * 100 > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "用户服务CPU使用率过高"
        description: "用户服务 {{ $labels.instance }} CPU使用率超过80%"
    
    - alert: UserServiceHighMemory
      expr: jvm_memory_used_bytes{job="user-service"} / jvm_memory_max_bytes{job="user-service"} * 100 > 85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "用户服务内存使用率过高"
        description: "用户服务 {{ $labels.instance }} 内存使用率超过85%"
    
    - alert: UserServiceHighErrorRate
      expr: rate(http_server_requests_seconds_count{job="user-service",status=~"5.."}[5m]) / rate(http_server_requests_seconds_count{job="user-service"}[5m]) * 100 > 5
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "用户服务错误率过高"
        description: "用户服务 {{ $labels.instance }} 5xx错误率超过5%"
    
    - alert: UserServiceSlowResponse
      expr: histogram_quantile(0.95, rate(http_server_requests_seconds_bucket{job="user-service"}[5m])) > 2
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "用户服务响应时间过长"
        description: "用户服务 {{ $labels.instance }} 95%响应时间超过2秒"
```

### 5.2 Grafana仪表板

#### 5.2.1 仪表板配置

```json
{
  "dashboard": {
    "id": null,
    "title": "用户服务监控仪表板",
    "tags": ["multishop", "user-service"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "服务状态",
        "type": "stat",
        "targets": [
          {
            "expr": "up{job=\"user-service\"}",
            "legendFormat": "{{instance}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "color": {
              "mode": "thresholds"
            },
            "thresholds": {
              "steps": [
                {"color": "red", "value": 0},
                {"color": "green", "value": 1}
              ]
            }
          }
        }
      },
      {
        "id": 2,
        "title": "QPS",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_server_requests_seconds_count{job=\"user-service\"}[5m])",
            "legendFormat": "{{instance}}"
          }
        ]
      },
      {
        "id": 3,
        "title": "响应时间",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.50, rate(http_server_requests_seconds_bucket{job=\"user-service\"}[5m]))",
            "legendFormat": "P50"
          },
          {
            "expr": "histogram_quantile(0.95, rate(http_server_requests_seconds_bucket{job=\"user-service\"}[5m]))",
            "legendFormat": "P95"
          },
          {
            "expr": "histogram_quantile(0.99, rate(http_server_requests_seconds_bucket{job=\"user-service\"}[5m]))",
            "legendFormat": "P99"
          }
        ]
      },
      {
        "id": 4,
        "title": "错误率",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_server_requests_seconds_count{job=\"user-service\",status=~\"4..\"}[5m])",
            "legendFormat": "4xx"
          },
          {
            "expr": "rate(http_server_requests_seconds_count{job=\"user-service\",status=~\"5..\"}[5m])",
            "legendFormat": "5xx"
          }
        ]
      },
      {
        "id": 5,
        "title": "JVM内存使用",
        "type": "graph",
        "targets": [
          {
            "expr": "jvm_memory_used_bytes{job=\"user-service\",area=\"heap\"}",
            "legendFormat": "Heap Used"
          },
          {
            "expr": "jvm_memory_max_bytes{job=\"user-service\",area=\"heap\"}",
            "legendFormat": "Heap Max"
          }
        ]
      },
      {
        "id": 6,
        "title": "数据库连接池",
        "type": "graph",
        "targets": [
          {
            "expr": "hikaricp_connections_active{job=\"user-service\"}",
            "legendFormat": "Active"
          },
          {
            "expr": "hikaricp_connections_idle{job=\"user-service\"}",
            "legendFormat": "Idle"
          },
          {
            "expr": "hikaricp_connections_max{job=\"user-service\"}",
            "legendFormat": "Max"
          }
        ]
      }
    ],
    "time": {
      "from": "now-1h",
      "to": "now"
    },
    "refresh": "30s"
  }
}
```

### 5.3 日志收集配置

#### 5.3.1 Filebeat配置

```yaml
# filebeat-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: filebeat-config
  namespace: multishop-user
data:
  filebeat.yml: |
    filebeat.inputs:
    - type: container
      paths:
        - /var/log/containers/*user-service*.log
      processors:
      - add_kubernetes_metadata:
          host: ${NODE_NAME}
          matchers:
          - logs_path:
              logs_path: "/var/log/containers/"
      - decode_json_fields:
          fields: ["message"]
          target: ""
          overwrite_keys: true
    
    output.elasticsearch:
      hosts: ["elasticsearch.logging.svc.cluster.local:9200"]
      index: "user-service-logs-%{+yyyy.MM.dd}"
      template.name: "user-service"
      template.pattern: "user-service-logs-*"
      template.settings:
        index.number_of_shards: 1
        index.number_of_replicas: 1
    
    logging.level: info
    logging.to_files: true
    logging.files:
      path: /var/log/filebeat
      name: filebeat
      keepfiles: 7
      permissions: 0644
```

#### 5.3.2 Logstash配置

```ruby
# logstash.conf
input {
  beats {
    port => 5044
  }
}

filter {
  if [kubernetes][container][name] == "user-service" {
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:thread}\] %{LOGLEVEL:level} %{DATA:logger} - %{GREEDYDATA:msg}" 
      }
    }
    
    date {
      match => [ "timestamp", "yyyy-MM-dd HH:mm:ss" ]
    }
    
    if [level] == "ERROR" {
      mutate {
        add_tag => [ "error" ]
      }
    }
    
    if [msg] =~ /Exception/ {
      mutate {
        add_tag => [ "exception" ]
      }
    }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "user-service-logs-%{+YYYY.MM.dd}"
  }
}
```

## 六、CI/CD流水线

### 6.1 GitLab CI/CD配置

#### 6.1.1 .gitlab-ci.yml

```yaml
# .gitlab-ci.yml
stages:
  - test
  - build
  - security
  - deploy-dev
  - deploy-test
  - deploy-prod

variables:
  MAVEN_OPTS: "-Dmaven.repo.local=$CI_PROJECT_DIR/.m2/repository"
  MAVEN_CLI_OPTS: "--batch-mode --errors --fail-at-end --show-version"
  DOCKER_DRIVER: overlay2
  DOCKER_TLS_CERTDIR: "/certs"

cache:
  paths:
    - .m2/repository/
    - target/

# 单元测试
test:
  stage: test
  image: maven:3.8.4-openjdk-11
  script:
    - mvn $MAVEN_CLI_OPTS test
  artifacts:
    reports:
      junit:
        - target/surefire-reports/TEST-*.xml
    paths:
      - target/site/jacoco/
  coverage: '/Total.*?([0-9]{1,3})%/'
  only:
    - merge_requests
    - main
    - develop

# 代码质量检查
code_quality:
  stage: test
  image: maven:3.8.4-openjdk-11
  script:
    - mvn $MAVEN_CLI_OPTS sonar:sonar
      -Dsonar.projectKey=$CI_PROJECT_NAME
      -Dsonar.host.url=$SONAR_HOST_URL
      -Dsonar.login=$SONAR_TOKEN
  only:
    - merge_requests
    - main

# 构建应用
build:
  stage: build
  image: maven:3.8.4-openjdk-11
  script:
    - mvn $MAVEN_CLI_OPTS clean package -DskipTests
  artifacts:
    paths:
      - target/*.jar
    expire_in: 1 hour
  only:
    - main
    - develop

# 构建Docker镜像
build_image:
  stage: build
  image: docker:20.10.16
  services:
    - docker:20.10.16-dind
  before_script:
    - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY
  script:
    - docker build -t $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA .
    - docker tag $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA $CI_REGISTRY_IMAGE:latest
    - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
    - docker push $CI_REGISTRY_IMAGE:latest
  dependencies:
    - build
  only:
    - main
    - develop

# 安全扫描
security_scan:
  stage: security
  image: aquasec/trivy:latest
  script:
    - trivy image --exit-code 1 --severity HIGH,CRITICAL $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA
  dependencies:
    - build_image
  allow_failure: false
  only:
    - main

# 部署到开发环境
deploy_dev:
  stage: deploy-dev
  image: bitnami/kubectl:latest
  script:
    - kubectl config use-context $KUBE_CONTEXT_DEV
    - kubectl set image deployment/user-service user-service=$CI_REGISTRY_IMAGE:$CI_COMMIT_SHA -n multishop-user-dev
    - kubectl rollout status deployment/user-service -n multishop-user-dev --timeout=300s
  environment:
    name: development
    url: https://dev-api.multishop.com/user
  dependencies:
    - build_image
  only:
    - develop

# 部署到测试环境
deploy_test:
  stage: deploy-test
  image: bitnami/kubectl:latest
  script:
    - kubectl config use-context $KUBE_CONTEXT_TEST
    - kubectl set image deployment/user-service user-service=$CI_REGISTRY_IMAGE:$CI_COMMIT_SHA -n multishop-user-test
    - kubectl rollout status deployment/user-service -n multishop-user-test --timeout=300s
  environment:
    name: testing
    url: https://test-api.multishop.com/user
  dependencies:
    - build_image
  when: manual
  only:
    - main

# 部署到生产环境
deploy_prod:
  stage: deploy-prod
  image: bitnami/kubectl:latest
  script:
    - kubectl config use-context $KUBE_CONTEXT_PROD
    - kubectl set image deployment/user-service user-service=$CI_REGISTRY_IMAGE:$CI_COMMIT_SHA -n multishop-user
    - kubectl rollout status deployment/user-service -n multishop-user --timeout=600s
  environment:
    name: production
    url: https://api.multishop.com/user
  dependencies:
    - build_image
  when: manual
  only:
    - main
```

### 6.2 Helm Chart配置

#### 6.2.1 Chart.yaml

```yaml
# Chart.yaml
apiVersion: v2
name: user-service
description: 多商户平台用户服务Helm Chart
type: application
version: 1.0.0
appVersion: "1.0.0"
keywords:
  - multishop
  - user-service
  - microservice
home: https://github.com/multishop/user-service
sources:
  - https://github.com/multishop/user-service
maintainers:
  - name: MultiShop Team
    email: dev@multishop.com
```

#### 6.2.2 values.yaml

```yaml
# values.yaml
replicaCount: 3

image:
  repository: harbor.multishop.com/multishop/user-service
  pullPolicy: Always
  tag: "latest"

imagePullSecrets:
  - name: harbor-secret

nameOverride: ""
fullnameOverride: ""

serviceAccount:
  create: true
  annotations: {}
  name: ""

podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8080"
  prometheus.io/path: "/actuator/prometheus"

podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1000
  fsGroup: 1000

securityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  capabilities:
    drop:
    - ALL

service:
  type: ClusterIP
  port: 80
  targetPort: 8080

ingress:
  enabled: true
  className: "nginx"
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /$2
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/rate-limit: "100"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
  hosts:
    - host: api.multishop.com
      paths:
        - path: /user(/|$)(.*)
          pathType: Prefix
  tls:
    - secretName: user-service-tls
      hosts:
        - api.multishop.com

resources:
  limits:
    cpu: 1000m
    memory: 2Gi
  requests:
    cpu: 500m
    memory: 1Gi

autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 20
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

nodeSelector:
  node-type: application

tolerations:
  - key: "application"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"

affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values:
            - user-service
        topologyKey: kubernetes.io/hostname

# 应用配置
config:
  spring:
    profiles:
      active: prod
  database:
    host: mysql-cluster.database.svc.cluster.local
    port: 3306
    name: multishop_user
    username: user_service
  redis:
    host: redis-cluster.cache.svc.cluster.local
    port: 6379

# 密钥配置
secrets:
  dbPassword: ""
  redisPassword: ""
  jwtSecret: ""
  encryptionKey: ""

# 监控配置
monitoring:
  enabled: true
  serviceMonitor:
    enabled: true
    interval: 30s
    path: /actuator/prometheus

# 日志配置
logging:
  level: INFO
  filebeat:
    enabled: true
```

#### 6.2.3 部署模板

```yaml
# templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "user-service.fullname" . }}
  labels:
    {{- include "user-service.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "user-service.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "user-service.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "user-service.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      containers:
        - name: {{ .Chart.Name }}
          securityContext:
            {{- toYaml .Values.securityContext | nindent 12 }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - name: http
              containerPort: 8080
              protocol: TCP
          env:
            - name: SPRING_PROFILES_ACTIVE
              value: {{ .Values.config.spring.profiles.active }}
            - name: DB_HOST
              value: {{ .Values.config.database.host }}
            - name: DB_PORT
              value: "{{ .Values.config.database.port }}"
            - name: DB_NAME
              value: {{ .Values.config.database.name }}
            - name: DB_USERNAME
              value: {{ .Values.config.database.username }}
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: {{ include "user-service.fullname" . }}-secret
                  key: db-password
            - name: REDIS_HOST
              value: {{ .Values.config.redis.host }}
            - name: REDIS_PORT
              value: "{{ .Values.config.redis.port }}"
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: {{ include "user-service.fullname" . }}-secret
                  key: redis-password
            - name: JWT_SECRET
              valueFrom:
                secretKeyRef:
                  name: {{ include "user-service.fullname" . }}-secret
                  key: jwt-secret
            - name: ENCRYPTION_KEY
              valueFrom:
                secretKeyRef:
                  name: {{ include "user-service.fullname" . }}-secret
                  key: encryption-key
          livenessProbe:
            httpGet:
              path: /actuator/health/liveness
              port: http
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
          readinessProbe:
            httpGet:
              path: /actuator/health/readiness
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
          startupProbe:
            httpGet:
              path: /actuator/health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 30
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
          volumeMounts:
            - name: config-volume
              mountPath: /app/config
            - name: logs-volume
              mountPath: /app/logs
      volumes:
        - name: config-volume
          configMap:
            name: {{ include "user-service.fullname" . }}-config
        - name: logs-volume
          emptyDir: {}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
```

## 七、灾难恢复与备份

### 7.1 备份策略

#### 7.1.1 数据备份

```yaml
# backup-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: user-service-backup
  namespace: multishop-user
spec:
  schedule: "0 2 * * *"  # 每天凌晨2点执行
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: mysql:8.0
            command:
            - /bin/bash
            - -c
            - |
              BACKUP_FILE="/backup/user_service_$(date +%Y%m%d_%H%M%S).sql"
              mysqldump -h $DB_HOST -u $DB_USER -p$DB_PASSWORD $DB_NAME > $BACKUP_FILE
              gzip $BACKUP_FILE
              
              # 上传到对象存储
              aws s3 cp ${BACKUP_FILE}.gz s3://multishop-backups/user-service/
              
              # 清理本地文件
              rm ${BACKUP_FILE}.gz
              
              # 清理过期备份（保留30天）
              aws s3 ls s3://multishop-backups/user-service/ | \
              while read -r line; do
                createDate=$(echo $line | awk '{print $1" "$2}')
                createDate=$(date -d "$createDate" +%s)
                olderThan=$(date -d "30 days ago" +%s)
                if [[ $createDate -lt $olderThan ]]; then
                  fileName=$(echo $line | awk '{print $4}')
                  if [[ $fileName != "" ]]; then
                    aws s3 rm s3://multishop-backups/user-service/$fileName
                  fi
                fi
              done
            env:
            - name: DB_HOST
              value: "mysql-cluster.database.svc.cluster.local"
            - name: DB_USER
              value: "backup_user"
            - name: DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: backup-secret
                  key: password
            - name: DB_NAME
              value: "multishop_user"
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: aws-secret
                  key: access-key-id
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: aws-secret
                  key: secret-access-key
            volumeMounts:
            - name: backup-storage
              mountPath: /backup
          volumes:
          - name: backup-storage
            emptyDir: {}
          restartPolicy: OnFailure
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
```

#### 7.1.2 配置备份

```bash
#!/bin/bash
# backup-configs.sh

NAMESPACE="multishop-user"
BACKUP_DIR="/backup/configs/$(date +%Y%m%d_%H%M%S)"
S3_BUCKET="s3://multishop-backups/configs"

mkdir -p $BACKUP_DIR

# 备份ConfigMaps
kubectl get configmaps -n $NAMESPACE -o yaml > $BACKUP_DIR/configmaps.yaml

# 备份Secrets
kubectl get secrets -n $NAMESPACE -o yaml > $BACKUP_DIR/secrets.yaml

# 备份Deployments
kubectl get deployments -n $NAMESPACE -o yaml > $BACKUP_DIR/deployments.yaml

# 备份Services
kubectl get services -n $NAMESPACE -o yaml > $BACKUP_DIR/services.yaml

# 备份Ingress
kubectl get ingress -n $NAMESPACE -o yaml > $BACKUP_DIR/ingress.yaml

# 备份HPA
kubectl get hpa -n $NAMESPACE -o yaml > $BACKUP_DIR/hpa.yaml

# 压缩备份文件
tar -czf $BACKUP_DIR.tar.gz -C $(dirname $BACKUP_DIR) $(basename $BACKUP_DIR)

# 上传到S3
aws s3 cp $BACKUP_DIR.tar.gz $S3_BUCKET/

# 清理本地文件
rm -rf $BACKUP_DIR $BACKUP_DIR.tar.gz

echo "配置备份完成: $S3_BUCKET/$(basename $BACKUP_DIR).tar.gz"
```

### 7.2 灾难恢复流程

#### 7.2.1 自动故障转移

```yaml
# failover-script.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: failover-script
  namespace: multishop-user
data:
  failover.sh: |
    #!/bin/bash
    
    # 检查主服务状态
    check_primary_health() {
        kubectl get pods -n multishop-user -l app=user-service --field-selector=status.phase=Running | grep -q user-service
        return $?
    }
    
    # 启动灾备服务
    activate_disaster_recovery() {
        echo "启动灾难恢复流程..."
        
        # 切换到灾备集群
        kubectl config use-context disaster-cluster
        
        # 恢复最新备份
        restore_latest_backup
        
        # 启动服务
        kubectl apply -f /disaster-recovery/user-service/
        
        # 等待服务就绪
        kubectl wait --for=condition=ready pod -l app=user-service -n multishop-user --timeout=300s
        
        # 更新DNS指向灾备环境
        update_dns_to_disaster_site
        
        echo "灾难恢复完成"
    }
    
    # 恢复最新备份
    restore_latest_backup() {
        LATEST_BACKUP=$(aws s3 ls s3://multishop-backups/user-service/ | sort | tail -n 1 | awk '{print $4}')
        aws s3 cp s3://multishop-backups/user-service/$LATEST_BACKUP /tmp/
        
        gunzip /tmp/$LATEST_BACKUP
        BACKUP_FILE=${LATEST_BACKUP%.gz}
        
        mysql -h disaster-mysql.database.svc.cluster.local -u root -p$MYSQL_ROOT_PASSWORD multishop_user < /tmp/$BACKUP_FILE
        
        rm /tmp/$BACKUP_FILE
    }
    
    # 更新DNS
    update_dns_to_disaster_site() {
        # 使用Route53或其他DNS服务更新记录
        aws route53 change-resource-record-sets --hosted-zone-id $HOSTED_ZONE_ID --change-batch '{
            "Changes": [{
                "Action": "UPSERT",
                "ResourceRecordSet": {
                    "Name": "api.multishop.com",
                    "Type": "A",
                    "TTL": 60,
                    "ResourceRecords": [{"Value": "'$DISASTER_SITE_IP'"}]
                }
            }]
        }'
    }
    
    # 主流程
    if ! check_primary_health; then
        echo "主服务不健康，启动故障转移..."
        activate_disaster_recovery
    else
        echo "主服务正常运行"
    fi
```

#### 7.2.2 手动恢复流程

```bash
#!/bin/bash
# manual-recovery.sh

set -e

BACKUP_DATE=${1:-$(date +%Y%m%d)}
NAMESPACE="multishop-user"

echo "开始手动恢复流程..."

# 1. 停止当前服务
echo "停止当前服务..."
kubectl scale deployment user-service --replicas=0 -n $NAMESPACE

# 2. 恢复数据库
echo "恢复数据库..."
BACKUP_FILE="user_service_${BACKUP_DATE}_*.sql.gz"
aws s3 cp s3://multishop-backups/user-service/$BACKUP_FILE /tmp/

gunzip /tmp/$BACKUP_FILE
RESTORED_FILE=${BACKUP_FILE%.gz}

# 创建恢复数据库
kubectl exec -it mysql-0 -n database -- mysql -u root -p$MYSQL_ROOT_PASSWORD -e "DROP DATABASE IF EXISTS multishop_user_recovery; CREATE DATABASE multishop_user_recovery;"

# 导入数据
kubectl exec -i mysql-0 -n database -- mysql -u root -p$MYSQL_ROOT_PASSWORD multishop_user_recovery < /tmp/$RESTORED_FILE

# 切换数据库
kubectl exec -it mysql-0 -n database -- mysql -u root -p$MYSQL_ROOT_PASSWORD -e "DROP DATABASE multishop_user; ALTER DATABASE multishop_user_recovery RENAME TO multishop_user;"

# 3. 恢复配置
echo "恢复配置..."
CONFIG_BACKUP="configs_${BACKUP_DATE}_*.tar.gz"
aws s3 cp s3://multishop-backups/configs/$CONFIG_BACKUP /tmp/

tar -xzf /tmp/$CONFIG_BACKUP -C /tmp/
kubectl apply -f /tmp/configs_${BACKUP_DATE}_*/

# 4. 重启服务
echo "重启服务..."
kubectl scale deployment user-service --replicas=3 -n $NAMESPACE

# 5. 验证服务
echo "验证服务..."
kubectl wait --for=condition=ready pod -l app=user-service -n $NAMESPACE --timeout=300s

# 6. 健康检查
echo "执行健康检查..."
for i in {1..10}; do
    if kubectl exec -n $NAMESPACE deployment/user-service -- curl -f http://localhost:8080/actuator/health; then
        echo "服务恢复成功"
        break
    fi
    echo "等待服务就绪... ($i/10)"
    sleep 30
done

# 7. 清理临时文件
rm -f /tmp/$RESTORED_FILE /tmp/$CONFIG_BACKUP
rm -rf /tmp/configs_${BACKUP_DATE}_*

echo "手动恢复流程完成"
```

## 八、性能优化

### 8.1 JVM调优

#### 8.1.1 JVM参数配置

```yaml
# jvm-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: jvm-config
  namespace: multishop-user
data:
  JVM_OPTS: |
    -Xms1g
    -Xmx2g
    -XX:+UseG1GC
    -XX:MaxGCPauseMillis=200
    -XX:+UseContainerSupport
    -XX:MaxRAMPercentage=75.0
    -XX:+HeapDumpOnOutOfMemoryError
    -XX:HeapDumpPath=/app/logs/heapdump.hprof
    -XX:+PrintGCDetails
    -XX:+PrintGCTimeStamps
    -XX:+PrintGCApplicationStoppedTime
    -Xloggc:/app/logs/gc.log
    -XX:+UseGCLogFileRotation
    -XX:NumberOfGCLogFiles=5
    -XX:GCLogFileSize=10M
    -Djava.security.egd=file:/dev/./urandom
    -Dspring.profiles.active=prod
    -Dfile.encoding=UTF-8
    -Duser.timezone=Asia/Shanghai
```

### 8.2 数据库连接池优化

#### 8.2.1 HikariCP配置

```yaml
# database-config.yaml
spring:
  datasource:
    hikari:
      # 连接池大小
      maximum-pool-size: 20
      minimum-idle: 5
      
      # 连接超时
      connection-timeout: 30000
      idle-timeout: 600000
      max-lifetime: 1800000
      
      # 连接测试
      connection-test-query: SELECT 1
      validation-timeout: 5000
      
      # 泄漏检测
      leak-detection-threshold: 60000
      
      # 连接池名称
      pool-name: UserServiceHikariCP
      
      # 自动提交
      auto-commit: true
      
      # 只读连接
      read-only: false
      
      # 事务隔离级别
      transaction-isolation: TRANSACTION_READ_COMMITTED
```

### 8.3 缓存优化

#### 8.3.1 Redis配置优化

```yaml
# redis-config.yaml
spring:
  redis:
    # 连接池配置
    lettuce:
      pool:
        max-active: 20
        max-idle: 10
        min-idle: 5
        max-wait: 5000ms
      
      # 集群配置
      cluster:
        refresh:
          adaptive: true
          period: 30s
      
      # 超时配置
      timeout: 5000ms
      
    # 序列化配置
    serialization:
      key-serializer: org.springframework.data.redis.serializer.StringRedisSerializer
      value-serializer: org.springframework.data.redis.serializer.GenericJackson2JsonRedisSerializer
      
    # 缓存配置
    cache:
      type: redis
      redis:
        time-to-live: 3600s
        cache-null-values: false
        use-key-prefix: true
        key-prefix: "user-service:"
```

## 九、安全加固

### 9.1 网络安全策略

#### 9.1.1 NetworkPolicy配置

```yaml
# networkpolicy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: user-service-netpol
  namespace: multishop-user
spec:
  podSelector:
    matchLabels:
      app: user-service
  policyTypes:
  - Ingress
  - Egress
  ingress:
  # 允许来自Ingress Controller的流量
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 8080
  # 允许来自其他微服务的流量
  - from:
    - namespaceSelector:
        matchLabels:
          name: multishop-order
    - namespaceSelector:
        matchLabels:
          name: multishop-product
    ports:
    - protocol: TCP
      port: 8080
  # 允许来自监控系统的流量
  - from:
    - namespaceSelector:
        matchLabels:
          name: monitoring
    ports:
    - protocol: TCP
      port: 8080
  egress:
  # 允许访问数据库
  - to:
    - namespaceSelector:
        matchLabels:
          name: database
    ports:
    - protocol: TCP
      port: 3306
  # 允许访问Redis
  - to:
    - namespaceSelector:
        matchLabels:
          name: cache
    ports:
    - protocol: TCP
      port: 6379
  # 允许DNS查询
  - to: []
    ports:
    - protocol: UDP
      port: 53
  # 允许HTTPS出站流量（第三方API调用）
  - to: []
    ports:
    - protocol: TCP
      port: 443
```

### 9.2 Pod安全策略

#### 9.2.1 PodSecurityPolicy配置

```yaml
# podsecuritypolicy.yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: user-service-psp
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
    - ALL
  volumes:
    - 'configMap'
    - 'emptyDir'
    - 'projected'
    - 'secret'
    - 'downwardAPI'
    - 'persistentVolumeClaim'
  runAsUser:
    rule: 'MustRunAsNonRoot'
  seLinux:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
  readOnlyRootFilesystem: true
  hostNetwork: false
  hostIPC: false
  hostPID: false
```

#### 9.2.2 SecurityContext配置

```yaml
# security-context.yaml
securityContext:
  runAsNonRoot: true
  runAsUser: 1000
  runAsGroup: 1000
  fsGroup: 1000
  seccompProfile:
    type: RuntimeDefault
  
containerSecurityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 1000
  runAsGroup: 1000
  capabilities:
    drop:
    - ALL
    add:
    - NET_BIND_SERVICE
  seccompProfile:
    type: RuntimeDefault
```

### 9.3 RBAC权限控制

#### 9.3.1 ServiceAccount配置

```yaml
# rbac.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: user-service-sa
  namespace: multishop-user
  labels:
    app: user-service
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: multishop-user
  name: user-service-role
rules:
- apiGroups: [""]
  resources: ["configmaps", "secrets"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: user-service-rolebinding
  namespace: multishop-user
subjects:
- kind: ServiceAccount
  name: user-service-sa
  namespace: multishop-user
roleRef:
  kind: Role
  name: user-service-role
  apiGroup: rbac.authorization.k8s.io
```

## 十、运维管理

### 10.1 日常运维脚本

#### 10.1.1 服务重启脚本

```bash
#!/bin/bash
# restart-service.sh

NAMESPACE="multishop-user"
DEPLOYMENT="user-service"

echo "开始重启用户服务..."

# 检查当前状态
echo "当前服务状态:"
kubectl get pods -n $NAMESPACE -l app=$DEPLOYMENT

# 执行滚动重启
echo "执行滚动重启..."
kubectl rollout restart deployment/$DEPLOYMENT -n $NAMESPACE

# 等待重启完成
echo "等待重启完成..."
kubectl rollout status deployment/$DEPLOYMENT -n $NAMESPACE --timeout=300s

# 验证服务状态
echo "验证服务状态..."
kubectl get pods -n $NAMESPACE -l app=$DEPLOYMENT

# 健康检查
echo "执行健康检查..."
for i in {1..5}; do
    if kubectl exec -n $NAMESPACE deployment/$DEPLOYMENT -- curl -f http://localhost:8080/actuator/health; then
        echo "服务重启成功"
        exit 0
    fi
    echo "等待服务就绪... ($i/5)"
    sleep 30
done

echo "服务重启失败，请检查日志"
exit 1
```

#### 10.1.2 日志查看脚本

```bash
#!/bin/bash
# view-logs.sh

NAMESPACE="multishop-user"
DEPLOYMENT="user-service"
LINES=${1:-100}
FOLLOW=${2:-false}

echo "查看用户服务日志..."

if [ "$FOLLOW" = "true" ]; then
    kubectl logs -f deployment/$DEPLOYMENT -n $NAMESPACE --tail=$LINES
else
    kubectl logs deployment/$DEPLOYMENT -n $NAMESPACE --tail=$LINES
fi
```

#### 10.1.3 性能监控脚本

```bash
#!/bin/bash
# monitor-performance.sh

NAMESPACE="multishop-user"
DEPLOYMENT="user-service"

echo "用户服务性能监控报告"
echo "========================"

# CPU和内存使用情况
echo "资源使用情况:"
kubectl top pods -n $NAMESPACE -l app=$DEPLOYMENT

# Pod状态
echo -e "\nPod状态:"
kubectl get pods -n $NAMESPACE -l app=$DEPLOYMENT -o wide

# HPA状态
echo -e "\n自动扩缩容状态:"
kubectl get hpa -n $NAMESPACE

# 服务端点
echo -e "\n服务端点:"
kubectl get endpoints -n $NAMESPACE $DEPLOYMENT

# 最近事件
echo -e "\n最近事件:"
kubectl get events -n $NAMESPACE --sort-by='.lastTimestamp' | tail -10
```

### 10.2 故障排查指南

#### 10.2.1 常见问题排查

```bash
#!/bin/bash
# troubleshoot.sh

NAMESPACE="multishop-user"
DEPLOYMENT="user-service"

echo "用户服务故障排查工具"
echo "===================="

# 1. 检查Pod状态
echo "1. 检查Pod状态:"
kubectl get pods -n $NAMESPACE -l app=$DEPLOYMENT

# 2. 检查Pod详细信息
echo -e "\n2. Pod详细信息:"
kubectl describe pods -n $NAMESPACE -l app=$DEPLOYMENT

# 3. 检查服务状态
echo -e "\n3. 服务状态:"
kubectl get svc -n $NAMESPACE $DEPLOYMENT

# 4. 检查Ingress状态
echo -e "\n4. Ingress状态:"
kubectl get ingress -n $NAMESPACE

# 5. 检查ConfigMap和Secret
echo -e "\n5. 配置状态:"
kubectl get configmaps,secrets -n $NAMESPACE

# 6. 检查最近日志
echo -e "\n6. 最近错误日志:"
kubectl logs deployment/$DEPLOYMENT -n $NAMESPACE --tail=50 | grep -i error

# 7. 检查资源使用
echo -e "\n7. 资源使用情况:"
kubectl top pods -n $NAMESPACE -l app=$DEPLOYMENT

# 8. 检查网络连接
echo -e "\n8. 网络连接测试:"
kubectl exec -n $NAMESPACE deployment/$DEPLOYMENT -- netstat -tlnp

# 9. 检查健康检查
echo -e "\n9. 健康检查:"
kubectl exec -n $NAMESPACE deployment/$DEPLOYMENT -- curl -f http://localhost:8080/actuator/health || echo "健康检查失败"

# 10. 检查数据库连接
echo -e "\n10. 数据库连接测试:"
kubectl exec -n $NAMESPACE deployment/$DEPLOYMENT -- curl -f http://localhost:8080/actuator/health/db || echo "数据库连接失败"
```

### 10.3 维护窗口管理

#### 10.3.1 维护模式脚本

```bash
#!/bin/bash
# maintenance-mode.sh

NAMESPACE="multishop-user"
DEPLOYMENT="user-service"
ACTION=${1:-"enable"}

if [ "$ACTION" = "enable" ]; then
    echo "启用维护模式..."
    
    # 创建维护页面
    kubectl create configmap maintenance-page -n $NAMESPACE --from-literal=index.html='
    <!DOCTYPE html>
    <html>
    <head>
        <title>系统维护中</title>
        <meta charset="utf-8">
    </head>
    <body>
        <h1>系统维护中</h1>
        <p>我们正在进行系统维护，预计30分钟后恢复服务。</p>
        <p>给您带来的不便，敬请谅解。</p>
    </body>
    </html>'
    
    # 部署维护页面
    kubectl apply -f - <<EOF
apiVersion: apps/v1
kind: Deployment
metadata:
  name: maintenance-page
  namespace: $NAMESPACE
spec:
  replicas: 2
  selector:
    matchLabels:
      app: maintenance-page
  template:
    metadata:
      labels:
        app: maintenance-page
    spec:
      containers:
      - name: nginx
        image: nginx:alpine
        ports:
        - containerPort: 80
        volumeMounts:
        - name: maintenance-content
          mountPath: /usr/share/nginx/html
      volumes:
      - name: maintenance-content
        configMap:
          name: maintenance-page
---
apiVersion: v1
kind: Service
metadata:
  name: maintenance-page
  namespace: $NAMESPACE
spec:
  selector:
    app: maintenance-page
  ports:
  - port: 80
    targetPort: 80
EOF
    
    # 更新Ingress指向维护页面
    kubectl patch ingress user-service-ingress -n $NAMESPACE --type='json' -p='[
        {
            "op": "replace",
            "path": "/spec/rules/0/http/paths/0/backend/service/name",
            "value": "maintenance-page"
        }
    ]'
    
    echo "维护模式已启用"
    
elif [ "$ACTION" = "disable" ]; then
    echo "禁用维护模式..."
    
    # 恢复Ingress指向正常服务
    kubectl patch ingress user-service-ingress -n $NAMESPACE --type='json' -p='[
        {
            "op": "replace",
            "path": "/spec/rules/0/http/paths/0/backend/service/name",
            "value": "user-service"
        }
    ]'
    
    # 删除维护页面
    kubectl delete deployment maintenance-page -n $NAMESPACE
    kubectl delete service maintenance-page -n $NAMESPACE
    kubectl delete configmap maintenance-page -n $NAMESPACE
    
    echo "维护模式已禁用"
    
else
    echo "用法: $0 [enable|disable]"
    exit 1
fi
```

## 十一、总结

### 11.1 部署架构优势

1. **高可用性**
   - 多副本部署，支持故障自动转移
   - 健康检查和自动重启机制
   - 跨可用区部署，提升容灾能力

2. **可扩展性**
   - 水平扩缩容支持，应对流量波动
   - 资源限制和请求配置，优化资源利用
   - 微服务架构，支持独立扩展

3. **安全性**
   - 多层安全防护，网络隔离
   - RBAC权限控制，最小权限原则
   - 镜像安全扫描，漏洞检测

4. **可维护性**
   - 标准化部署流程，简化运维
   - 完善的监控和日志系统
   - 自动化CI/CD流水线

### 11.2 运维最佳实践

1. **监控告警**
   - 建立完善的监控指标体系
   - 设置合理的告警阈值
   - 定期检查和优化告警规则

2. **备份恢复**
   - 定期执行数据备份
   - 验证备份文件完整性
   - 定期演练恢复流程

3. **性能优化**
   - 持续监控性能指标
   - 定期进行性能测试
   - 根据业务增长调整资源配置

4. **安全加固**
   - 定期更新镜像和依赖
   - 执行安全扫描和渗透测试
   - 保持安全策略的时效性

### 11.3 后续改进方向

1. **服务网格深化**
   - 完善Istio配置，提升服务治理能力
   - 实现更细粒度的流量控制
   - 加强服务间通信的安全性

2. **可观测性增强**
   - 引入分布式链路追踪
   - 完善业务指标监控
   - 建立智能化运维平台

3. **成本优化**
   - 实现更精准的资源调度
   - 引入Spot实例降低成本
   - 优化存储和网络费用

4. **自动化提升**
   - 完善自动化测试覆盖
   - 实现零停机部署
   - 建立自愈能力

本部署设计文档为用户模块提供了完整的生产级部署方案，涵盖了从容器化到监控运维的各个方面，确保服务的高可用、高性能和高安全性。